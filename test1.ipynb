{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating delay for stream: Stream_1, size: 6496, period: 1000.0, deadline: 8225.0, pcp: 6\n",
      "  At hop ES_1 -> Switch_1, output port 1, link_id Link_1:\n",
      "    Higher-priority streams: []\n",
      "    Same-priority streams: ['Stream_2']\n",
      "    Lower-priority streams: []\n",
      "    Calculated b_H = 0, b_C_j = 0, b = 6496, l_min = 6496, l_L = 0, r = 1000000000, r_H = 0\n",
      "    Per-hop delay at ES_1 -> Switch_1 for stream Stream_1: 6.496e-06\n",
      "  At hop Switch_1 -> ES_2, output port 2, link_id Link_2:\n",
      "    Higher-priority streams: []\n",
      "    Same-priority streams: ['Stream_2', 'Stream_5', 'Stream_7', 'Stream_8']\n",
      "    Lower-priority streams: []\n",
      "    Calculated b_H = 0, b_C_j = 21632, b = 6496, l_min = 6496, l_L = 0, r = 1000000000, r_H = 0\n",
      "    Per-hop delay at Switch_1 -> ES_2 for stream Stream_1: 2.8128e-05\n",
      "Total delay for stream Stream_1: 3.4624e-05\n",
      "\n",
      "Calculating delay for stream: Stream_2, size: 6248, period: 1000.0, deadline: 10156.0, pcp: 6\n",
      "  At hop ES_1 -> Switch_1, output port 1, link_id Link_1:\n",
      "    Higher-priority streams: []\n",
      "    Same-priority streams: ['Stream_1']\n",
      "    Lower-priority streams: []\n",
      "    Calculated b_H = 0, b_C_j = 0, b = 6248, l_min = 6248, l_L = 0, r = 1000000000, r_H = 0\n",
      "    Per-hop delay at ES_1 -> Switch_1 for stream Stream_2: 6.248e-06\n",
      "  At hop Switch_1 -> ES_2, output port 2, link_id Link_2:\n",
      "    Higher-priority streams: []\n",
      "    Same-priority streams: ['Stream_1', 'Stream_5', 'Stream_7', 'Stream_8']\n",
      "    Lower-priority streams: []\n",
      "    Calculated b_H = 0, b_C_j = 21880, b = 6248, l_min = 6248, l_L = 0, r = 1000000000, r_H = 0\n",
      "    Per-hop delay at Switch_1 -> ES_2 for stream Stream_2: 2.8128e-05\n",
      "Total delay for stream Stream_2: 3.4376e-05\n",
      "\n",
      "Calculating delay for stream: Stream_3, size: 5552, period: 2000.0, deadline: 13729.0, pcp: 6\n",
      "  At hop ES_2 -> Switch_1, output port 1, link_id Link_2:\n",
      "    Higher-priority streams: []\n",
      "    Same-priority streams: ['Stream_4']\n",
      "    Lower-priority streams: []\n",
      "    Calculated b_H = 0, b_C_j = 0, b = 5552, l_min = 5552, l_L = 0, r = 1000000000, r_H = 0\n",
      "    Per-hop delay at ES_2 -> Switch_1 for stream Stream_3: 5.552e-06\n",
      "  At hop Switch_1 -> Switch_2, output port 3, link_id Link_5:\n",
      "    Higher-priority streams: []\n",
      "    Same-priority streams: []\n",
      "    Lower-priority streams: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 338\u001b[0m\n\u001b[1;32m    335\u001b[0m streams_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreams.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your actual file path\u001b[39;00m\n\u001b[1;32m    337\u001b[0m graph \u001b[38;5;241m=\u001b[39m Graph()\n\u001b[0;32m--> 338\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_worst_case_delay_for_all_streams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopology_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtopology_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstreams_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 328\u001b[0m, in \u001b[0;36mGraph.compute_worst_case_delay_for_all_streams\u001b[0;34m(self, topology_file, streams_file, output_file, verbose)\u001b[0m\n\u001b[1;32m    326\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriterow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStreamName, MaxE2E(us), Deadline(us), Path\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stream_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_paths\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 328\u001b[0m     wcd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_worst_case_delay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstream_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(wcd\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1e6\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreams[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeadline\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_paths[stream_name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 277\u001b[0m, in \u001b[0;36mGraph.compute_worst_case_delay\u001b[0;34m(self, stream_name, verbose)\u001b[0m\n\u001b[1;32m    274\u001b[0m temp_list \u001b[38;5;241m=\u001b[39m [(s_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m header_size) \u001b[38;5;28;01mfor\u001b[39;00m s_name \u001b[38;5;129;01min\u001b[39;00m same_priority_streams\n\u001b[1;32m    275\u001b[0m      \u001b[38;5;28;01mfor\u001b[39;00m s_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreams \u001b[38;5;28;01mif\u001b[39;00m s_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m s_name]\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# exclude min burst from temp_list\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m temp_list\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtemp_list\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m [\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    278\u001b[0m b_C_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(temp_list)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# I think is the correct way to calculate b_C_j. It selects the max burst of the same priority streams, thus ensuring the worst-case scenario. This is same as the eq 10 in the project description. \u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Calculate b_C_j as the max of burst of any same-priority interfering stream\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# b_C_j = max(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Debug output for r_H, b_H, l_L and b_C_j\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        # Initialize an empty NetworkX graph\n",
    "        self.G = nx.Graph()\n",
    "        self.stream_paths = {}  # Dictionary to store paths for each stream\n",
    "        # Dictionary to store queue assignments for each output port\n",
    "        self.queue_assignments = {}\n",
    "\n",
    "    def load_from_csv(self, topology_file):\n",
    "        \"\"\"\n",
    "        Loads a topology from a CSV file and populates the graph with nodes and edges.\n",
    "\n",
    "        Args:\n",
    "            topology_file (str): Path to the topology CSV file.\n",
    "        \"\"\"\n",
    "        devices = []\n",
    "        links = []\n",
    "\n",
    "        # Use the csv module to read the file\n",
    "        with open(topology_file, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for fields in reader:\n",
    "                if fields[0].lower() == 'es' or fields[0].lower() == 'sw':  # Device entry (SW or ES)\n",
    "                    devices.append(fields)\n",
    "                elif fields[0].lower() == 'link':  # Link entry\n",
    "                    links.append(fields)\n",
    "\n",
    "        # Add nodes (SW: Switch, ES: End System)\n",
    "        for device in devices:\n",
    "            device_type = device[0].strip()\n",
    "            device_name = device[1].strip()\n",
    "            self.G.add_node(device_name, type=device_type)\n",
    "\n",
    "        # Add edges (LINK)\n",
    "        for link in links:\n",
    "            link_id = link[1].strip()\n",
    "            source_device = link[2].strip()\n",
    "            source_port = int(link[3].strip())\n",
    "            destination_device = link[4].strip()\n",
    "            destination_port = int(link[5].strip())\n",
    "\n",
    "            # Add the edge (link) between source and destination devices, preserving direction information\n",
    "            self.G.add_edge(\n",
    "                source_device, destination_device, link_id=link_id,\n",
    "                source_port=source_port, destination_port=destination_port,\n",
    "                source_device=source_device, destination_device=destination_device\n",
    "            )\n",
    "\n",
    "    def load_streams(self, streams_file):\n",
    "        \"\"\"\n",
    "        Loads stream information from a CSV file.\n",
    "\n",
    "        Args:\n",
    "            streams_file (str): Path to the streams CSV file.\n",
    "        \"\"\"\n",
    "        streams = []\n",
    "        with open(streams_file, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                streams.append({\n",
    "                    'pcp': int(row[0]),\n",
    "                    'name': row[1].strip(),\n",
    "                    'type': row[2].strip(),\n",
    "                    'source': row[3].strip(),\n",
    "                    'destination': row[4].strip(),\n",
    "                    'size': int(row[5])*8,\n",
    "                    'period': float(row[6]),\n",
    "                    'deadline': float(row[7])\n",
    "                })\n",
    "        self.streams = streams\n",
    "        \n",
    "        # debug print\n",
    "        # for stream in self.streams:\n",
    "        #     print(stream)\n",
    "\n",
    "\n",
    "    def find_shortest_path(self, node_a, node_b):\n",
    "        \"\"\"\n",
    "        Finds the shortest path between two nodes using Dijkstra's algorithm.\n",
    "\n",
    "        Args:\n",
    "            node_a (str): The source node.\n",
    "            node_b (str): The destination node.\n",
    "\n",
    "        Returns:\n",
    "            path (list): The shortest path from node_a to node_b as a list of nodes.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Verify that nodes exist in the graph\n",
    "            if node_a not in self.G or node_b not in self.G:\n",
    "                print(\n",
    "                    f\"One or both of the nodes ({node_a}, {node_b}) are not in the graph.\")\n",
    "                return None\n",
    "\n",
    "            # Use Dijkstra's algorithm to find the shortest path without weights\n",
    "            path = nx.shortest_path(self.G, source=node_a, target=node_b)\n",
    "            return path\n",
    "        except nx.NetworkXNoPath:\n",
    "            print(f\"No path exists between {node_a} and {node_b}.\")\n",
    "            return None\n",
    "        except nx.NodeNotFound:\n",
    "            print(\n",
    "                f\"One or both of the nodes ({node_a}, {node_b}) are not in the graph.\")\n",
    "            return None\n",
    "\n",
    "    def calculate_all_paths(self):\n",
    "        \"\"\"\n",
    "        Calculates and stores the shortest paths for all streams, adjusting for the direction of traversal.\n",
    "        \"\"\"\n",
    "        for stream in self.streams:\n",
    "            source = stream['source']\n",
    "            destination = stream['destination']\n",
    "            path = self.find_shortest_path(source, destination)\n",
    "            if path:\n",
    "                annotated_path = []\n",
    "                for i in range(len(path) - 1):\n",
    "                    current_node = path[i]\n",
    "                    next_node = path[i + 1]\n",
    "                    \n",
    "                    # Get edge data between current node and next node\n",
    "                    edge_data = self.G.get_edge_data(current_node, next_node)\n",
    "                    link_id = edge_data['link_id']\n",
    "                    \n",
    "                    # Determine direction of traversal and select appropriate ports\n",
    "                    if current_node == edge_data['source_device']:\n",
    "                        source_port = edge_data['source_port']\n",
    "                        destination_port = edge_data['destination_port']\n",
    "                    else:\n",
    "                        # If reversed, swap ports\n",
    "                        source_port = edge_data['destination_port']\n",
    "                        destination_port = edge_data['source_port']\n",
    "\n",
    "                    # Annotate the current node details for the path\n",
    "                    annotated_path.append(f\"{current_node}:{link_id}:{source_port}\")\n",
    "                \n",
    "                # Append final destination node without outgoing link data\n",
    "                annotated_path.append(f\"{destination}\")\n",
    "                \n",
    "                # Store annotated path in `stream_paths` dictionary\n",
    "                self.stream_paths[stream['name']] = '->'.join(annotated_path)\n",
    "                \n",
    "                # Debug output\n",
    "                # print(f\"{stream['name']} from {source} to {destination}: {self.stream_paths[stream['name']]}\")\n",
    "            else:\n",
    "                print(f\"No path found for Stream {stream['name']} from {source} to {destination}\")\n",
    "\n",
    "\n",
    "\n",
    "    def assign_queues(self):\n",
    "        \"\"\"\n",
    "        Assigns shaped queues for each combination of priority level and upstream source for every output port.\n",
    "        \"\"\"\n",
    "        for stream_name, path in self.stream_paths.items():\n",
    "            stream = next(s for s in self.streams if s['name'] == stream_name)\n",
    "            pcp = stream['pcp']\n",
    "            path_nodes = path.split('->')\n",
    "            for i in range(len(path_nodes) - 1):\n",
    "                current_node = path_nodes[i].split(':')[0]\n",
    "                previous_node = path_nodes[i - 1].split(':')[0] if i > 0 else 'N/A'\n",
    "                next_node = path_nodes[i + 1].split(':')[0]\n",
    "                \n",
    "                # Get edge data between current and next node\n",
    "                edge_data = self.G.get_edge_data(current_node, next_node)\n",
    "                \n",
    "                # Determine output port based on traversal direction\n",
    "                if current_node == edge_data['source_device']:\n",
    "                    output_port = edge_data['source_port']\n",
    "                else:\n",
    "                    output_port = edge_data['destination_port']\n",
    "\n",
    "                # Create a queue assignment key for each (current_node, previous_node, output_port, pcp)\n",
    "                key = (current_node, previous_node, output_port, pcp)\n",
    "\n",
    "                if key not in self.queue_assignments:\n",
    "                    self.queue_assignments[key] = []\n",
    "                self.queue_assignments[key].append(stream_name)\n",
    "\n",
    "\n",
    "    def compute_worst_case_delay(self, stream_name, verbose=False):\n",
    "        \"\"\"\n",
    "        Computes the worst-case per-hop delay for a stream over its path.\n",
    "\n",
    "        Args:\n",
    "            stream_name (str): The name of the stream.\n",
    "\n",
    "        Returns:\n",
    "            float: The computed worst-case delay for the stream.\n",
    "        \"\"\"\n",
    "        if stream_name not in self.stream_paths:\n",
    "            print(f\"No path found for stream {stream_name}.\")\n",
    "            return None\n",
    "\n",
    "        header_size = 0  # Ethernet frame header size\n",
    "        path = self.stream_paths[stream_name].split('->')\n",
    "        stream = next(s for s in self.streams if s['name'] == stream_name)\n",
    "        b = stream['size'] + header_size  # Include Ethernet frame overhead\n",
    "        r = b / stream['period']\n",
    "        l_min = b  # Minimum frame size\n",
    "        r_link = 1000000000  # Assume 1 Gbps link rate for now\n",
    "        total_delay = 0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nCalculating delay for stream: {stream_name}, size: {b}, period: {stream['period']}, deadline: {stream['deadline']}, pcp: {stream['pcp']}\")\n",
    "\n",
    "        for i in range(len(path) - 1):\n",
    "            current_node = path[i].split(':')[0]\n",
    "            next_node = path[i + 1].split(':')[0]\n",
    "            edge_data = self.G.get_edge_data(current_node, next_node)\n",
    "\n",
    "            # Determine output port based on traversal direction\n",
    "            if current_node == edge_data['source_device']:\n",
    "                output_port = edge_data['source_port']\n",
    "            else:\n",
    "                output_port = edge_data['destination_port']\n",
    "\n",
    "            # Gather all streams at this output port across priority levels\n",
    "            all_interfering_streams = [\n",
    "                s_name for k, streams in self.queue_assignments.items()\n",
    "                if k[0] == current_node and k[2] == output_port for s_name in streams\n",
    "            ]\n",
    "\n",
    "            # Organize interfering streams by priority level relative to `stream['pcp']`\n",
    "            higher_priority_streams = []\n",
    "            same_priority_streams = []\n",
    "            lower_priority_streams = []\n",
    "\n",
    "            for s_name in all_interfering_streams:\n",
    "\n",
    "                s_data = next((s for s in self.streams if s['name'] == s_name and s['name'] != stream_name), None)\n",
    "                if s_data:\n",
    "                    if s_data['pcp'] > stream['pcp']:\n",
    "                        higher_priority_streams.append(s_name)\n",
    "                    elif s_data['pcp'] == stream['pcp']:\n",
    "                        same_priority_streams.append(s_name)\n",
    "                    else:\n",
    "                        lower_priority_streams.append(s_name)\n",
    "\n",
    "            # Debug output for all interfering streams organized by priority\n",
    "            if verbose:\n",
    "                print(f\"  At hop {current_node} -> {next_node}, output port {output_port}, link_id {edge_data['link_id']}:\")\n",
    "                print(\"    Higher-priority streams:\", higher_priority_streams)\n",
    "                print(\"    Same-priority streams:\", same_priority_streams)\n",
    "                print(\"    Lower-priority streams:\", lower_priority_streams)\n",
    "\n",
    "            # Calculate r_H: Total rate of higher-priority streams\n",
    "            r_H = sum(\n",
    "                (s_data['size'] + header_size) / s_data['period']\n",
    "                for s_name in higher_priority_streams\n",
    "                for s_data in self.streams if s_data['name'] == s_name)\n",
    "\n",
    "            # Calculate b_H once per hop as the total burst size of all higher-priority streams\n",
    "            b_H = sum(\n",
    "                (s_data['size'] + header_size) for s_name in higher_priority_streams\n",
    "                for s_data in self.streams if s_data['name'] == s_name)\n",
    "\n",
    "            # Calculate l_L once per hop as the maximum size of lower-priority streams\n",
    "            l_L = max(\n",
    "                [(s_data['size'] + header_size) for s_name in lower_priority_streams\n",
    "                for s_data in self.streams if s_data['name'] == s_name] or [0])\n",
    "\n",
    "\n",
    "            # TODO\n",
    "            #######################################################################################\n",
    "            # I dont think this is the correct way to calculate b_C_j. Why would we sum the burst of all same-priority interfering streams to get the worst-case scenario?\n",
    "            # Calculate b_C_j as the sum of burst of any same-priority interfering stream\n",
    "            # b_C_j = sum(\n",
    "            #     [(s_data['size'] + header_size) for s_name in same_priority_streams\n",
    "            #      for s_data in self.streams if s_data['name'] == s_name])\n",
    "\n",
    "            # Patrick solution\n",
    "            temp_list = [(s_data['size'] + header_size) for s_name in same_priority_streams\n",
    "                 for s_data in self.streams if s_data['name'] == s_name]\n",
    "            # exclude min burst from temp_list\n",
    "            temp_list.remove(min(temp_list) or [0])\n",
    "            b_C_j = sum(temp_list)\n",
    "\n",
    "            # I think is the correct way to calculate b_C_j. It selects the max burst of the same priority streams, thus ensuring the worst-case scenario. This is same as the eq 10 in the project description. \n",
    "            # Calculate b_C_j as the max of burst of any same-priority interfering stream\n",
    "            # b_C_j = max(\n",
    "            #     [(s_data['size'] + header_size) for s_name in same_priority_streams\n",
    "            #      for s_data in self.streams if s_data['name'] == s_name] or [0])\n",
    "            #######################################################################################\n",
    "\n",
    "            # Debug output for r_H, b_H, l_L and b_C_j\n",
    "            if verbose:\n",
    "                print(f\"    Calculated b_H = {b_H}, b_C_j = {b_C_j}, b = {b}, l_min = {l_min}, l_L = {l_L}, r = {r_link}, r_H = {r_H}\")\n",
    "\n",
    "            # Calculate the per-hop delay using the worst-case conditions at this hop\n",
    "            try:\n",
    "                per_hop_delay = (b_H + b_C_j + (b - l_min) + l_L) / (r_link - r_H) + (l_min / r_link)\n",
    "            except:\n",
    "                return None\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"    Per-hop delay at {current_node} -> {next_node} for stream {stream_name}: {per_hop_delay}\")\n",
    "            \n",
    "            total_delay += per_hop_delay\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Total delay for stream {stream_name}: {total_delay}\")\n",
    "        # return f'{stream_name}, {round(total_delay*1e6,3)}, {stream[\"deadline\"]}, {self.stream_paths[stream_name]}'\n",
    "        return total_delay\n",
    "\n",
    "\n",
    "    def compute_worst_case_delay_for_all_streams(self,topology_file, streams_file, output_file='output.csv', verbose=False):\n",
    "        \"\"\"\n",
    "        Computes the worst-case per-hop delay for all streams over their respective paths.\n",
    "        \"\"\"\n",
    "        # Build the graph\n",
    "        self.load_from_csv(topology_file)\n",
    "\n",
    "        # Load streams\n",
    "        self.load_streams(streams_file)\n",
    "\n",
    "        # calculate stream paths\n",
    "        self.calculate_all_paths()\n",
    "\n",
    "        # assign queues\n",
    "        self.assign_queues()\n",
    "\n",
    "        with open(output_file, 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow('StreamName, MaxE2E(us), Deadline(us), Path'.split(','))\n",
    "            for stream_name in self.stream_paths.keys():\n",
    "                wcd = self.compute_worst_case_delay(stream_name,verbose=verbose)\n",
    "                writer.writerow(f'{stream_name}, {round(wcd*1e6,3)}, {self.streams[0][\"deadline\"]}, {self.stream_paths[stream_name]}'.split(','))\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "topology_file = 'topology.csv'  # Replace with your actual file path\n",
    "streams_file = 'streams.csv'  # Replace with your actual file path\n",
    "\n",
    "graph = Graph()\n",
    "graph.compute_worst_case_delay_for_all_streams(topology_file = topology_file, streams_file = streams_file, output_file='output.csv', verbose=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    # Example usage\n",
    "    topology_file = 'topology.csv'  # Replace with your actual file path\n",
    "    streams_file = 'streams.csv'  # Replace with your actual file path\n",
    "\n",
    "    graph = Graph()\n",
    "    graph.compute_worst_case_delay_for_all_streams(topology_file = topology_file, streams_file = streams_file, output_file='output.csv', verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
